{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from emoatlas import EmoScores\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"localdb\"\n",
    "valence_dir = \"Valence_Metrics\"\n",
    "language='eng'\n",
    "\n",
    "emos = EmoScores()\n",
    "emosita = EmoScores(language='italian')\n",
    "\n",
    "# Folders of interest\n",
    "folders_of_interest = ['climate', 'math', 'misinformation_health','gwarming']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data={}\n",
    "\n",
    "for folder in folders_of_interest:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        fmnts=[]\n",
    "        texts=[]\n",
    "\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if language=='ita':\n",
    "            if '(ITA)' in file_path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        json_obj = json.loads(line)\n",
    "                        fmnts.append(json_obj['fmnt']['syntactic'])\n",
    "                        texts.append(' '.join(json_obj['lemmatized_test']))\n",
    "                models_data[filename.rstrip('.jsonl')] = { 'Network': emosita.combine_edgelists(fmnts),\n",
    "                                         'Texts': ' '.join(texts)\n",
    "                }              \n",
    "        if language=='eng':\n",
    "            if '(ITA)' not in file_path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        json_obj = json.loads(line)\n",
    "                        fmnts.append(json_obj['fmnt']['syntactic'])\n",
    "                        texts.append(' '.join(json_obj['lemmatized_test']))\n",
    "                models_data[filename.rstrip('.jsonl')] = { 'Network': emos.combine_edgelists(fmnts),\n",
    "                                         'Texts': ' '.join(texts)\n",
    "                }              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(network_data):\n",
    "    G = nx.Graph()\n",
    "    for edge in network_data:\n",
    "        G.add_edge(edge[0], edge[1], weight=edge[2])\n",
    "    return G\n",
    "\n",
    "def compute_metrics(G, text):\n",
    "    strength = dict(nx.degree(G, weight='weight'))\n",
    "    degree = dict(nx.degree(G))\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    \n",
    "    # Compute word frequencies\n",
    "    word_frequencies = defaultdict(int)\n",
    "    words = text.split()  # Split text into words\n",
    "    \n",
    "    for word in words:\n",
    "        word_frequencies[word] += 1  # Count frequency of each word \n",
    "    word_frequencies=dict(word_frequencies)\n",
    "    return strength, degree, closeness, word_frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_models():\n",
    "    results = {model:{} for model in models_data}\n",
    "    all_tf_words = {}\n",
    "    all_degree_words={}\n",
    "\n",
    "    for model in models_data:\n",
    "        print('doing ', model)\n",
    "        G = create_graph(models_data[model]['Network'])\n",
    "        strength, degree, closeness, tf = compute_metrics(G, models_data[model]['Texts'])\n",
    "        \n",
    "        results[model]['strength'] = strength\n",
    "        results[model]['degree'] = degree\n",
    "        results[model]['closeness'] = closeness\n",
    "        results[model]['tf'] = tf\n",
    "\n",
    "        all_tf_words[model] = set(degree.keys())\n",
    "\n",
    "    common_words = set.intersection(*all_tf_words.values())\n",
    "    \n",
    "    # Filter out words not used in all models\n",
    "    for model in results:\n",
    "        results[model]['strength'] = {word: value for word, value in results[model]['strength'].items() if word in common_words}\n",
    "        results[model]['degree'] = {word: value for word, value in results[model]['degree'].items() if word in common_words}\n",
    "        results[model]['closeness'] = {word: value for word, value in results[model]['closeness'].items() if word in common_words}\n",
    "        results[model]['tf'] = {word: value for word, value in results[model]['tf'].items() if word in common_words}\n",
    "\n",
    "    all_words_used = set().union(*all_degree_words.values(), *all_tf_words.values())\n",
    "    words_not_in_all_models = all_words_used - common_words\n",
    "    if words_not_in_all_models:\n",
    "        print(\"Words not used by every model:\", words_not_in_all_models)\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = analyze_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dictionary_keys(data):\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    # Find common words across all models and all inner dictionaries\n",
    "    all_word_sets = [\n",
    "        set(inner_dict.keys())\n",
    "        for model_data in data.values()\n",
    "        for inner_dict in model_data.values()\n",
    "    ]\n",
    "    common_words = set.intersection(*all_word_sets)\n",
    "    \n",
    "    print(f\"Number of common words across all models: {len(common_words)}\")\n",
    "    \n",
    "    for model, model_data in data.items():\n",
    "        print(f\"Model: {model}\")\n",
    "        \n",
    "        # Create a new dictionary for this model with only common words\n",
    "        cleaned_data[model] = {\n",
    "            inner_dict_type: {word: value for word, value in inner_dict.items() if word in common_words}\n",
    "            for inner_dict_type, inner_dict in model_data.items()\n",
    "        }\n",
    "        \n",
    "        # Print the length of each metric after cleaning\n",
    "        for metric_name, metric_values in cleaned_data[model].items():\n",
    "            print(f\"  Metric: {metric_name}, Len: {len(metric_values)}\")\n",
    "        \n",
    "        print(\"---\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "results=clean_dictionary_keys(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordstoremove={'math':['math','anxiety','matematica','ansia'],\n",
    "               'climate':['climate','change','cambiamento','climatico'],\n",
    "               'gwarming':['global','warming','riscaldamento','globale'],\n",
    "               'misinformation_health':['misinformation','health','disinformazione','salute','bufala','teoria','complotto','information','theory','conspiracy'],}\n",
    "\n",
    "def remove_keys(d):\n",
    "    if isinstance(d, dict):\n",
    "        # Filter out keys named 'climate' or 'change'\n",
    "        d = {k: remove_keys(v) for k, v in d.items() if k.lower() not in wordstoremove[folders_of_interest[0]]}\n",
    "    return d\n",
    "\n",
    "results2=remove_keys(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_differences(results):\n",
    "\n",
    "    import math\n",
    "    # Calculate Euclidean distance for 'strength', 'degree', 'closeness', and 'tf' for each model\n",
    "    for model in results:\n",
    "        other_models = [m for m in results if m != model]\n",
    "        \n",
    "        for node in results[model]['strength']:\n",
    "            distances = [(results[model]['strength'][node] - results[other_model]['strength'].get(node, 0))**2 for other_model in other_models]\n",
    "            euclidean_distance = math.sqrt(sum(distances))\n",
    "            results[model].setdefault('diff_strength', {})[node] = euclidean_distance\n",
    "        \n",
    "        for node in results[model]['degree']:\n",
    "            distances = [(results[model]['degree'][node] - results[other_model]['degree'].get(node, 0))**2 for other_model in other_models]\n",
    "            euclidean_distance = math.sqrt(sum(distances))\n",
    "            results[model].setdefault('diff_degree', {})[node] = euclidean_distance\n",
    "\n",
    "        for node in results[model]['closeness']:\n",
    "            distances = [(results[model]['closeness'][node] - results[other_model]['closeness'].get(node, 0))**2 for other_model in other_models]\n",
    "            euclidean_distance = math.sqrt(sum(distances))\n",
    "            results[model].setdefault('diff_closeness', {})[node] = euclidean_distance\n",
    "        \n",
    "        for word in results[model]['tf']:\n",
    "            distances = [(results[model]['tf'][word] - results[other_model]['tf'].get(word, 0))**2 for other_model in other_models]\n",
    "            euclidean_distance = math.sqrt(sum(distances))\n",
    "            results[model].setdefault('diff_tf', {})[word] = euclidean_distance\n",
    "\n",
    "    # Example output for checking\n",
    "    for model in results:\n",
    "        print(f\"Model: {model}\")\n",
    "        if 'believe' in results[model]['diff_strength']:\n",
    "            print(f\"Euclidean Distance Strength for 'believe': {results[model]['diff_strength']['believe']}\")\n",
    "        if 'believe' in results[model]['diff_degree']:\n",
    "            print(f\"Euclidean Distance Degree for 'believe': {results[model]['diff_degree']['believe']}\")\n",
    "        if 'believe' in results[model]['diff_closeness']:\n",
    "            print(f\"Euclidean Distance Closeness for 'believe': {results[model]['diff_closeness']['believe']}\")\n",
    "        if 'believe' in results[model]['diff_tf']:\n",
    "            print(f\"Euclidean Distance TF for 'believe': {results[model]['diff_tf']['believe']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_differences=compute_euclidean_differences(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_differences.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standardized_dataframe(results):\n",
    "    # Step 1-4: Create initial DataFrame and calculate z-scores (unchanged)\n",
    "    df = pd.concat({k: pd.DataFrame(v['strength'].items(), columns=['node', 'strength']) for k, v in results.items()}, axis=1)\n",
    "\n",
    "    display(df[df.isna().any(axis=1)])\n",
    "\n",
    "    # Delete rows with NaN values\n",
    "    df = df.dropna()\n",
    "\n",
    "    for model in results:\n",
    "        df[(model, 'closeness')] = df[(model, 'node')].map(results[model]['closeness'])\n",
    "        df[(model, 'degree')] = df[(model, 'node')].map(results[model]['degree'])\n",
    "        df[(model, 'tf')] = df[(model, 'node')].map(results[model]['tf'])\n",
    "    \n",
    "    for model in results:\n",
    "        #df[(model, 's')] = (df[(model, 'strength')]/1000).round(1)\n",
    "        df[(model, 'd')] = (df[(model, 'degree')]/100).round(1)\n",
    "        df[(model, 'f')] = (df[(model, 'tf')]/1000).round(1)\n",
    "        df[(model, 'c')] = df[(model, 'closeness')].round(2)\n",
    "\n",
    "    for model in results:\n",
    "        #df[(model, 'Ds')] = df[(model, 'node')].map(results[model]['diff_strength'])\n",
    "        df[(model, 'Dd')] = df[(model, 'node')].map(results[model]['diff_degree'])\n",
    "        df[(model, 'Df')] = df[(model, 'node')].map(results[model]['diff_tf'])\n",
    "        df[(model, 'Dc')] = df[(model, 'node')].map(results[model]['diff_closeness'])\n",
    "\n",
    "    for model in results:\n",
    "        #df[(model, 'Ds')] = (df[(model, 'Ds')]/1000).round(1)\n",
    "        df[(model, 'Dd')] = (df[(model, 'Dd')]/100).round(1)\n",
    "        df[(model, 'Df')] = (df[(model, 'Df')]/1000).round(1)\n",
    "        df[(model, 'Dc')] = df[(model, 'Dc')].round(2)\n",
    "   \n",
    "    \n",
    "    for model in results:\n",
    "        for metric in ['degree', 'tf', 'closeness']: #add strength here if you need it\n",
    "            mean = df[(model, metric)].mean()\n",
    "            std = df[(model, metric)].std()\n",
    "            if std == 0:\n",
    "                df[(model, f'{metric}_zscore')] = 0\n",
    "            else:\n",
    "                df[(model, f'{metric}_zscore')] = (df[(model, metric)] - mean) / std\n",
    "\n",
    "    # Step 5: Sort each model's data by nodes\n",
    "    sorted_dfs = {}\n",
    "    for model in results:\n",
    "        model_columns = [(model, col) for col in df.columns.levels[1] if col in ['node','d','f','c', 'degree_zscore', 'tf_zscore','closeness_zscore','Dd','Df','Dc']] #add s and strength_zscore here if you need it\n",
    "        sorted_dfs[model] = df[model_columns].sort_values(by=(model, 'node')).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    ## Step 6: Calculate cosine similarities\n",
    "    #models = list(results.keys())\n",
    "    #for i, model1 in enumerate(models):\n",
    "    #    for j, model2 in enumerate(models):\n",
    "    #        if i < j:  # Only calculate for unique pairs\n",
    "    #            vec1 = sorted_dfs[model1][[\n",
    "    #                (model1, 'strength_zscore'),\n",
    "    #                #(model1, 'closeness_zscore'),\n",
    "    #                (model1, 'degree_zscore'),\n",
    "    #                (model1, 'tf_zscore')\n",
    "    #            ]].values\n",
    "    #            vec2 = sorted_dfs[model2][[\n",
    "    #                (model2, 'strength_zscore'),\n",
    "    #                #(model2, 'closeness_zscore'),\n",
    "    #                (model2, 'degree_zscore'),\n",
    "    #                (model2, 'tf_zscore')\n",
    "    #            ]].values\n",
    "    #            \n",
    "    #            cosine_sim = np.array([\n",
    "    #                abs(1 - cosine(v1, v2) if not np.isnan(v1).any() and not np.isnan(v2).any() else np.nan)\n",
    "    #                for v1, v2 in zip(vec1, vec2)\n",
    "    #            ])\n",
    "    #            \n",
    "    #            sorted_dfs[model1][(model1, f'S{model2[0]}')] = cosine_sim\n",
    "    #            sorted_dfs[model2][(model2, f'S{model1[0]}')] = cosine_sim\n",
    "        # Step 6: Calculate average cosine similarity\n",
    "    models = list(results.keys())\n",
    "    for i, model1 in enumerate(models):\n",
    "        cosine_sims = []\n",
    "        for j, model2 in enumerate(models):\n",
    "            if i != j:  # Calculate for all pairs except self\n",
    "                vec1 = sorted_dfs[model1][[\n",
    "                    (model1, 'closeness_zscore'),\n",
    "                    (model1, 'degree_zscore'),\n",
    "                    (model1, 'tf_zscore')\n",
    "                ]].values\n",
    "                vec2 = sorted_dfs[model2][[\n",
    "                    (model2, 'closeness_zscore'),\n",
    "                    (model2, 'degree_zscore'),\n",
    "                    (model2, 'tf_zscore')\n",
    "                ]].values\n",
    "                \n",
    "                cosine_sim = np.array([\n",
    "                    abs(1 - cosine(v1, v2) if not np.isnan(v1).any() and not np.isnan(v2).any() else np.nan)\n",
    "                    for v1, v2 in zip(vec1, vec2)\n",
    "                ])\n",
    "                \n",
    "                cosine_sims.append(cosine_sim)\n",
    "        \n",
    "        avg_cosine_sim = np.nanmean(cosine_sims, axis=0)\n",
    "        sorted_dfs[model1][(model1, 'AvgS')] = avg_cosine_sim          \n",
    "\n",
    "    #for model in results:\n",
    "    #    model_columns = [(model, col) for col in ['node', 'strength_zscore', 'closeness_zscore', 'degree_zscore', 'tf_zscore']]\n",
    "    # Step 7: Sort each model's data by strength_zscore\n",
    "\n",
    "    for model in results:\n",
    "        sorted_dfs[model] = sorted_dfs[model].sort_values(by=(model, 'f'), ascending=False).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    # Step 8: Concatenate sorted DataFrames\n",
    "    df_sorted = pd.concat(sorted_dfs.values(), axis=1)\n",
    "\n",
    "    for model in results:\n",
    "        for metric in ['closeness_zscore', 'degree_zscore', 'tf_zscore']:\n",
    "            del df_sorted[(model, metric)]  \n",
    "\n",
    "\n",
    "    for column in df_sorted.columns:\n",
    "        if column[1] != 'AvgS':\n",
    "            df_sorted[column] = df_sorted[column].astype(str)\n",
    "\n",
    "\n",
    "    # Split the dataframe into two parts\n",
    "    models = list(results.keys())\n",
    "    print(models)\n",
    "    models = ['gpt-3.5','Haiku','mistral-7b','Llama-3-8B']\n",
    "    df_first_two = df_sorted.loc[:, models[:2]]\n",
    "    df_second_two = df_sorted.loc[:, models[2:]]\n",
    "\n",
    "    captiondictionary={'climate':'Climate Change',\n",
    "                       'gwarming':'Global warming',\n",
    "                       'math': 'Math anxiety',\n",
    "                       'misinformation_health': 'Misinformation in health'}\n",
    "    captionlanguage={'ita':'Italian',\n",
    "                     'eng':'English'}\n",
    "\n",
    "    # Function to create LaTeX table\n",
    "    def create_latex_table(df, caption):\n",
    "        caption = f\"Top 20 nodes (based on frequency) for the {captiondictionary[folders_of_interest[0]]} topic in {captionlanguage[language]}. \"\n",
    "        caption += f\"Columns show node name (node), degree (d, divided by 100), frequency (f, divided by 1000), \"\n",
    "        caption += f\"and their Euclidean differences (Dd, Df, Dc) to other models. \"\n",
    "        caption += \"AvgS represents the average cosine similarity with other models.\"\n",
    "        caption += \"Note: d and Dd are shown after division by 100, while f and Df are shown after division by 1000.\"\n",
    "\n",
    "\n",
    "        latex_table = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\setlength{\\\\tabcolsep}{4pt}\\n\\\\footnotesize\\n\"\n",
    "        latex_table += df.head(20).to_latex(index=False, float_format=\"%.2f\")\n",
    "        latex_table += f\"\\\\caption{{{caption}}}\\n\\\\label{{tab:mytable}}\\n\\\\end{{table}}\"\n",
    "        return latex_table\n",
    "\n",
    "    # Create LaTeX tables\n",
    "    latex_table1 = create_latex_table(df_first_two, f\"Top 20 nodes (based on f) for {folders_of_interest[0]}\")\n",
    "    latex_table2 = create_latex_table(df_second_two, f\"Top 20 nodes (based on f) for  {folders_of_interest[0]}\")\n",
    "    #latex_output = df_sorted.head(20).to_latex(index=False, float_format=f\"%.1f\")\n",
    "    print(latex_table1, '\\n')\n",
    "    print(latex_table2)\n",
    "\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(create_standardized_dataframe(euclidean_differences).head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoatlas_ennemesino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
