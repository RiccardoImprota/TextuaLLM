{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoatlas import EmoScores\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from emoatlas.resources import _valences\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "emos=EmoScores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = \"localdb\"\n",
    "folder = 'math'\n",
    "model = 'Haiku'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model(filename):\n",
    "    match = re.search(r'_(.*?)_', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valence_sets(tfmn, language='english'):\n",
    "    \n",
    "    positive, negative, ambivalent = _valences(language)\n",
    "    all_nodes = set(node for edge in tfmn for node in edge[:2])\n",
    "\n",
    "    positive_nodes = all_nodes.intersection(positive)\n",
    "    negative_nodes = all_nodes.intersection(negative)\n",
    "    neutral_nodes = all_nodes - (positive_nodes | negative_nodes )\n",
    "\n",
    "    return positive_nodes, negative_nodes, neutral_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_elements_in_sets(set1, set2, set3):\n",
    "    # Calculate total number of elements\n",
    "    total_elements = len(set1) + len(set2) + len(set3)\n",
    "    \n",
    "    # Convert sets to lists for easy manipulation\n",
    "    lists = [list(set1), list(set2), list(set3)]\n",
    "    \n",
    "    # Perform swapping operations\n",
    "    for _ in range(10 * total_elements):\n",
    "        # Pick 2 random sets without repetition\n",
    "        set_indices = random.sample(range(3), 2)\n",
    "        \n",
    "        # Pick a random word from each set\n",
    "        word1 = random.choice(lists[set_indices[0]])\n",
    "        word2 = random.choice(lists[set_indices[1]])\n",
    "        \n",
    "        # Only swap if the words don't already exist in the other list\n",
    "        if word1 not in lists[set_indices[1]] and word2 not in lists[set_indices[0]]:\n",
    "            lists[set_indices[0]].remove(word1)\n",
    "            lists[set_indices[1]].remove(word2)\n",
    "            lists[set_indices[0]].append(word2)\n",
    "            lists[set_indices[1]].append(word1)\n",
    "    \n",
    "    # Convert lists back to sets\n",
    "    return set(lists[0]), set(lists[1]), set(lists[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = os.path.join(base_output_dir, folder)\n",
    "        \n",
    "for filename in os.listdir(model_paths):\n",
    "    print(filename.rstrip('.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = os.path.join(base_output_dir, folder, model+'.jsonl')\n",
    "\n",
    "\n",
    "with open(input_folder_path, 'r') as file:\n",
    "    fmnts=[]\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        fmnts.append(data['fmnt']['syntactic'])\n",
    "    weighted_fmnt = emos.combine_edgelists(fmnts)\n",
    "\n",
    "    positive, negative, neutral = get_valence_sets(weighted_fmnt, language='english')\n",
    "    random_positive, random_negative, random_neutral = swap_elements_in_sets(positive, negative, neutral)\n",
    "\n",
    "    print(len(positive),positive)\n",
    "    print(len(random_positive),random_positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = os.path.join(base_output_dir, folder, model+'.jsonl')\n",
    "\n",
    "def get_random_edges():\n",
    "    with open(input_folder_path, 'r') as file:\n",
    "        fmnts=[]\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            fmnts.append(data['fmnt']['syntactic'])\n",
    "        weighted_fmnt = emos.combine_edgelists(fmnts)\n",
    "\n",
    "        positive, negative, neutral = get_valence_sets(weighted_fmnt, language='english')\n",
    "        random_positive, random_negative, random_neutral = swap_elements_in_sets(positive, negative, neutral)\n",
    "\n",
    "        def get_random_valence(word):\n",
    "                            if word in random_positive:\n",
    "                                return 'positive'\n",
    "                            elif word in random_negative:\n",
    "                                return 'negative'\n",
    "                            else:\n",
    "                                return 'neutral'\n",
    "\n",
    "                    \n",
    "    # Weighted network analysis\n",
    "    random_weights = {\n",
    "        'pos_pos': 0, 'pos_neutral': 0, 'neg_neutral': 0,\n",
    "        'neg_neg': 0, 'neutral_neutral': 0, 'pos_neg': 0,\n",
    "        'weight_pos_pos': 0, 'weight_pos_neutral': 0, 'weight_neg_neutral': 0,\n",
    "        'weight_neg_neg': 0, 'weight_neutral_neutral': 0, 'weight_pos_neg': 0\n",
    "    }\n",
    "    \n",
    "    for node1, node2, weight in weighted_fmnt:\n",
    "        random_valence1, random_valence2 = get_random_valence(node1), get_random_valence(node2)\n",
    "    \n",
    "        # Random distribution\n",
    "        if random_valence1 == 'positive' and random_valence2 == 'positive':\n",
    "            random_weights['pos_pos'] += 1\n",
    "            random_weights['weight_pos_pos'] += weight\n",
    "        elif (random_valence1 == 'positive' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'positive'):\n",
    "            random_weights['pos_neutral'] += 1\n",
    "            random_weights['weight_pos_neutral'] += weight\n",
    "        elif (random_valence1 == 'negative' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'negative'):\n",
    "            random_weights['neg_neutral'] += 1\n",
    "            random_weights['weight_neg_neutral'] += weight\n",
    "        elif random_valence1 == 'negative' and random_valence2 == 'negative':\n",
    "            random_weights['neg_neg'] += 1\n",
    "            random_weights['weight_neg_neg'] += weight\n",
    "        elif random_valence1 == 'neutral' and random_valence2 == 'neutral':\n",
    "            random_weights['neutral_neutral'] += 1\n",
    "            random_weights['weight_neutral_neutral'] += weight\n",
    "        elif (random_valence1 == 'negative' and random_valence2 == 'positive') or (random_valence1 == 'positive' and random_valence2 == 'negative'):\n",
    "            random_weights['pos_neg'] += 1\n",
    "            random_weights['weight_pos_neg'] += weight\n",
    "        \n",
    "\n",
    "        return(random_weights)\n",
    "    \n",
    "print(get_random_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pos_weights=[]\n",
    "\n",
    "for i in range(100):\n",
    "    pos_pos_weights.append(get_random_edges()['pos_pos'])\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(pos_pos_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_stats(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)\n",
    "    n = len(data)\n",
    "    print(mean,std,n)\n",
    "\n",
    "    # Z-value for 95% confidence\n",
    "    z_value = 1.96\n",
    "\n",
    "    # Calculate margin of error\n",
    "    margin_of_error = z_value * (std / sqrt(n))\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    confidence_interval = (mean - margin_of_error, mean + margin_of_error)\n",
    "\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Standard Deviation: {std}\")\n",
    "    print(f\"Sample Size: {n}\")\n",
    "    print(f\"95% Confidence Interval: {confidence_interval}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "calculate_stats(pos_pos_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoatlas_ennemesino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
