{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoatlas import EmoScores\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from emoatlas.resources import _valences\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "emos=EmoScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = \"localdb\"\n",
    "valence_dir = \"Valence_Metrics\"\n",
    "\n",
    "# Folders of interest\n",
    "folders_of_interest = ['climate', 'math', 'misinformation_health','gwarming']\n",
    "folders_of_interest = ['math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model(filename):\n",
    "    match = re.search(r'_(.*?)_', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valence_sets(weighted_fmnt, language='english'):\n",
    "    \n",
    "    positive, negative, ambivalent = _valences(language)\n",
    "\n",
    "    all_nodes = set(node for edge in weighted_fmnt for node in edge[:2])\n",
    "\n",
    "    positive_nodes = all_nodes.intersection(positive)\n",
    "    negative_nodes = all_nodes.intersection(negative)\n",
    "    neutral_nodes = all_nodes - (positive_nodes | negative_nodes )\n",
    "\n",
    "    return positive_nodes, negative_nodes, neutral_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_elements_in_sets(set1, set2, set3):\n",
    "    # Calculate total number of elements\n",
    "    total_elements = len(set1) + len(set2) + len(set3)\n",
    "    \n",
    "    # Convert sets to lists for easy manipulation\n",
    "    lists = [list(set1), list(set2), list(set3)]\n",
    "    \n",
    "    # Perform swapping operations\n",
    "    for _ in range(10 * total_elements):\n",
    "        # Pick 2 random sets without repetition\n",
    "        set_indices = random.sample(range(3), 2)\n",
    "        \n",
    "        # Pick a random word from each set\n",
    "        word1 = random.choice(lists[set_indices[0]])\n",
    "        word2 = random.choice(lists[set_indices[1]])\n",
    "        \n",
    "        # Only swap if the words don't already exist in the other list\n",
    "        if word1 not in lists[set_indices[1]] and word2 not in lists[set_indices[0]]:\n",
    "            lists[set_indices[0]].remove(word1)\n",
    "            lists[set_indices[1]].remove(word2)\n",
    "            lists[set_indices[0]].append(word2)\n",
    "            lists[set_indices[1]].append(word1)\n",
    "    \n",
    "    # Convert lists back to sets\n",
    "    return set(lists[0]), set(lists[1]), set(lists[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders_of_interest:\n",
    "        input_folder_path = os.path.join(base_output_dir, folder)\n",
    "        valence_output_path = os.path.join(valence_dir, folder)\n",
    "\n",
    "        for filename in os.listdir(input_folder_path):\n",
    "            print(filename.rstrip('.jsonl'))\n",
    "\n",
    "            file_path = os.path.join(input_folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                fmnts=[]\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    fmnts.append(data['fmnt']['syntactic'])\n",
    "            \n",
    "            weighted_fmnt = emos.combine_edgelists(fmnts)\n",
    "            if ('ITA') in filename:\n",
    "                  language='italian'\n",
    "            else:\n",
    "                  language='english'\n",
    "                  \n",
    "                  \n",
    "            \n",
    "            positive, negative, neutral = get_valence_sets(weighted_fmnt, language=language)\n",
    "            random_positive, random_negative, random_neutral = swap_elements_in_sets(positive, negative, neutral)\n",
    "            print(len(positive),len(negative),len(neutral))\n",
    "            print(len(random_positive),len(random_negative),len(random_neutral))\n",
    "            print(positive)\n",
    "            print(random_positive)\n",
    "\n",
    "            def get_random_valence(word):\n",
    "                        if word in random_positive:\n",
    "                            return 'positive'\n",
    "                        elif word in random_negative:\n",
    "                            return 'negative'\n",
    "                        else:\n",
    "                            return 'neutral'\n",
    "                        \n",
    "            # Weighted network analysis\n",
    "            random_weights = {\n",
    "                'pos_pos': 0,\n",
    "                'neg_neg': 0, 'pos_neg': 0,\n",
    "            }\n",
    "        \n",
    "            for node1, node2, weight in weighted_fmnt:\n",
    "                random_valence1, random_valence2 = get_random_valence(node1), get_random_valence(node2)\n",
    "        \n",
    "                # Random distribution\n",
    "                if random_valence1 == 'positive' and random_valence2 == 'positive':\n",
    "                    random_weights['pos_pos'] += 1\n",
    "                elif random_valence1 == 'negative' and random_valence2 == 'negative':\n",
    "                    random_weights['neg_neg'] += 1\n",
    "                elif (random_valence1 == 'negative' and random_valence2 == 'positive') or (random_valence1 == 'positive' and random_valence2 == 'negative'):\n",
    "                    random_weights['pos_neg'] += 1\n",
    "\n",
    "            print(random_weights)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_loop_valences():\n",
    "    #positive, negative, neutral = get_valence_sets(weighted_network, language=language)\n",
    "    for folder in folders_of_interest:\n",
    "        input_folder_path = os.path.join(base_output_dir, folder)\n",
    "        valence_output_path = os.path.join(valence_dir, folder)\n",
    "\n",
    "        for filename in os.listdir(input_folder_path):\n",
    "            print(filename.rstrip('.jsonl'))\n",
    "\n",
    "            file_path = os.path.join(input_folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                fmnts=[]\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    fmnts.append(data['fmnt']['syntactic'])\n",
    "            \n",
    "            weighted_fmnt = emos.combine_edgelists(fmnts)\n",
    "            if ('ITA') in filename:\n",
    "                  language='italian'\n",
    "            else:\n",
    "                  language='english'\n",
    "                  \n",
    "                  \n",
    "            \n",
    "            positive, negative, neutral = get_valence_sets(weighted_fmnt, language=language)\n",
    "\n",
    "            output_file_path = os.path.join(valence_output_path, f\"{filename.rstrip('.jsonl')}_valence.jsonl\")\n",
    "            with open(output_file_path, 'a') as out_file:\n",
    "                for i in range(1000):\n",
    "                    if i % 50 == 0:\n",
    "                        print(i)\n",
    "                    random_positive, random_negative, random_neutral = swap_elements_in_sets(positive, negative, neutral)\n",
    "                    \n",
    "                    def get_random_valence(word):\n",
    "                        if word in random_positive:\n",
    "                            return 'positive'\n",
    "                        elif word in random_negative:\n",
    "                            return 'negative'\n",
    "                        else:\n",
    "                            return 'neutral'\n",
    "                    \n",
    "                    # Weighted network analysis\n",
    "                    random_weights = {\n",
    "                        'pos_pos': 0, 'pos_neutral': 0, 'neg_neutral': 0,\n",
    "                        'neg_neg': 0, 'neutral_neutral': 0, 'pos_neg': 0,\n",
    "                        'weight_pos_pos': 0, 'weight_pos_neutral': 0, 'weight_neg_neutral': 0,\n",
    "                        'weight_neg_neg': 0, 'weight_neutral_neutral': 0, 'weight_pos_neg': 0\n",
    "                    }\n",
    "        \n",
    "                    for node1, node2, weight in weighted_fmnt:\n",
    "                        random_valence1, random_valence2 = get_random_valence(node1), get_random_valence(node2)\n",
    "            \n",
    "                        # Random distribution\n",
    "                        if random_valence1 == 'positive' and random_valence2 == 'positive':\n",
    "                            random_weights['pos_pos'] += 1\n",
    "                            random_weights['weight_pos_pos'] += weight\n",
    "                        elif (random_valence1 == 'positive' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'positive'):\n",
    "                            random_weights['pos_neutral'] += 1\n",
    "                            random_weights['weight_pos_neutral'] += weight\n",
    "                        elif (random_valence1 == 'negative' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'negative'):\n",
    "                            random_weights['neg_neutral'] += 1\n",
    "                            random_weights['weight_neg_neutral'] += weight\n",
    "                        elif random_valence1 == 'negative' and random_valence2 == 'negative':\n",
    "                            random_weights['neg_neg'] += 1\n",
    "                            random_weights['weight_neg_neg'] += weight\n",
    "                        elif random_valence1 == 'neutral' and random_valence2 == 'neutral':\n",
    "                            random_weights['neutral_neutral'] += 1\n",
    "                            random_weights['weight_neutral_neutral'] += weight\n",
    "                        elif (random_valence1 == 'negative' and random_valence2 == 'positive') or (random_valence1 == 'positive' and random_valence2 == 'negative'):\n",
    "                            random_weights['pos_neg'] += 1\n",
    "                            random_weights['weight_pos_neg'] += weight\n",
    "                        \n",
    "            \n",
    "            \n",
    "                    json.dump(random_weights, out_file)\n",
    "                    out_file.write('\\n')\n",
    "                    out_file.flush()\n",
    "                    \n",
    "export_loop_valences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valence_metrics(weighted_network, model, topic, language='english'):\n",
    "    \n",
    "    positive, negative, neutral = get_valence_sets(weighted_network, language=language)\n",
    "    random_positive, random_negative, random_neutral = swap_elements_in_sets(positive, negative, neutral)\n",
    "\n",
    "    def get_valence(word):\n",
    "        if word in positive:\n",
    "            return 'positive'\n",
    "        elif word in negative:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "        \n",
    "    def get_random_valence(word):\n",
    "        if word in random_positive:\n",
    "            return 'positive'\n",
    "        elif word in random_negative:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    # Weighted network analysis\n",
    "    weight_pos_pos, weight_pos_neutral, weight_pos_neg,weight_neg_neutral, weight_neg_neg, weight_neutral_neutral = 0, 0, 0, 0, 0, 0\n",
    "    random_weight_pos_pos, random_weight_pos_neutral, random_weight_pos_neg, random_weight_neg_neutral, random_weight_neg_neg, random_weight_neutral_neutral = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    for node1, node2, weight in weighted_network:\n",
    "        valence1, valence2 = get_valence(node1), get_valence(node2)\n",
    "        random_valence1, random_valence2 = get_random_valence(node1), get_random_valence(node2)\n",
    "        \n",
    "        # Normal distribution\n",
    "        if valence1 == 'positive' and valence2 == 'positive':\n",
    "            weight_pos_pos += weight\n",
    "        elif (valence1 == 'positive' and valence2 == 'neutral') or (valence1 == 'neutral' and valence2 == 'positive'):\n",
    "            weight_pos_neutral += weight\n",
    "        elif (valence1 == 'negative' and valence2 == 'neutral') or (valence1 == 'neutral' and valence2 == 'negative'):\n",
    "            weight_neg_neutral += weight\n",
    "        elif valence1 == 'negative' and valence2 == 'negative':\n",
    "            weight_neg_neg += weight\n",
    "        elif valence1 == 'neutral' and valence2 == 'neutral':\n",
    "            weight_neutral_neutral += weight\n",
    "        elif (valence1 == 'negative' and valence2 == 'positive') or (valence1 == 'positive' and valence2 == 'negative'):\n",
    "            weight_pos_neg += weight\n",
    "        \n",
    "        # Random distribution\n",
    "        if random_valence1 == 'positive' and random_valence2 == 'positive':\n",
    "            random_weight_pos_pos += weight\n",
    "        elif (random_valence1 == 'positive' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'positive'):\n",
    "            random_weight_pos_neutral += weight\n",
    "        elif (random_valence1 == 'negative' and random_valence2 == 'neutral') or (random_valence1 == 'neutral' and random_valence2 == 'negative'):\n",
    "            random_weight_neg_neutral += weight\n",
    "        elif random_valence1 == 'negative' and random_valence2 == 'negative':\n",
    "            random_weight_neg_neg += weight\n",
    "        elif random_valence1 == 'neutral' and random_valence2 == 'neutral':\n",
    "            random_weight_neutral_neutral += weight\n",
    "        elif (random_valence1 == 'negative' and random_valence2 == 'positive') or (random_valence1 == 'positive' and random_valence2 == 'negative'):\n",
    "            random_weight_pos_neg += weight\n",
    "    \n",
    "    print(\"\\nWeighted Network Metrics:\")\n",
    "    print(f\"Total weight of edges between positive nodes: {weight_pos_pos}\")\n",
    "    print(f\"Total weight of edges between positive and neutral nodes: {weight_pos_neutral}\")\n",
    "    print(f\"Total weight of edges between negative and neutral nodes: {weight_neg_neutral}\")\n",
    "    print(f\"Total weight of edges between negative nodes: {weight_neg_neg}\")\n",
    "    print(f\"Total weight of edges between neutral nodes: {weight_neutral_neutral}\")\n",
    "    print(f\"Total weight of edges between contrasting nodes: {weight_pos_neg}\")\n",
    "    \n",
    "    print(\"\\nRandom Distribution Metrics:\")\n",
    "    print(f\"Total weight of edges between positive nodes: {random_weight_pos_pos}\")\n",
    "    print(f\"Total weight of edges between positive and neutral nodes: {random_weight_pos_neutral}\")\n",
    "    print(f\"Total weight of edges between negative and neutral nodes: {random_weight_neg_neutral}\")\n",
    "    print(f\"Total weight of edges between negative nodes: {random_weight_neg_neg}\")\n",
    "    print(f\"Total weight of edges between neutral nodes: {random_weight_neutral_neutral}\")\n",
    "    print(f\"Total weight of edges between contrasting nodes: {random_weight_pos_neg}\")\n",
    "    \n",
    "    # Calculate z-scores\n",
    "    normal_weights = [weight_pos_pos, weight_pos_neutral, weight_neg_neutral, weight_neg_neg, weight_neutral_neutral,weight_pos_neg]\n",
    "    random_weights = [random_weight_pos_pos, random_weight_pos_neutral, random_weight_neg_neutral, random_weight_neg_neg, random_weight_neutral_neutral,random_weight_pos_neg]\n",
    "    \n",
    "    z_scores = stats.zscore(normal_weights + random_weights)[:len(normal_weights)]\n",
    "    \n",
    "    print(\"\\nZ-scores (Normal vs Random):\")\n",
    "    for label, z_score in zip(['Positive-Positive', 'Positive-Neutral', 'Negative-Neutral', 'Negative-Negative', 'Neutral-Neutral','Positive-Negative'], z_scores):\n",
    "        print(f\"{label}: {z_score:.2f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    labels = ['Positive-Positive', 'Positive-Neutral', 'Negative-Neutral', 'Negative-Negative', 'Neutral-Neutral','Positive-Negative']\n",
    "    normal_sizes = normal_weights\n",
    "    random_sizes = random_weights\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    ax1.pie(normal_sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Normal Valence Distribution')\n",
    "    \n",
    "    ax2.pie(random_sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Random Valence Distribution')\n",
    "    \n",
    "    plt.suptitle(f'Distribution of Edge Weights by Node Valence for model {model} and topic {topic}: Normal vs Random')\n",
    "    plt.show()\n",
    "# Example usage:\n",
    "\n",
    "#valence_metrics(w_fmnt,'Haiku','climate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in folders_of_interest:\n",
    "    folder_path = os.path.join(base_output_dir, folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        #print(filename.rstrip('.jsonl'))\n",
    "\n",
    "        if '(ITA)' in filename:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                \n",
    "                fmnts=[]\n",
    "\n",
    "                for line in file:\n",
    "                    data = json.loads(line)\n",
    "                    fmnts.append(data['fmnt']['syntactic'])\n",
    "                weighted_fmnt = emos.combine_edgelists(fmnts)\n",
    "                print(len(weighted_fmnt))\n",
    "                print(weighted_fmnt[0])\n",
    "                valence_metrics(weighted_fmnt,filename.rstrip('.jsonl'),folder,language='italian')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoatlas_ennemesino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
